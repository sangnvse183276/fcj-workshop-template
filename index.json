[{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/5-workshop/5.2-auth-setup/","title":"Configuring Google Cloud &amp; Amazon Cognito","tags":[],"description":"","content":" ‚öôÔ∏è Objective: Set up a Google Cloud Platform project to obtain OAuth credentials and configure Amazon Cognito User Pool as the centralized identity management system.\n1. Configuring Google Cloud Platform (GCP) To allow users to sign in with Gmail, we first need to create a project on Google Cloud and request OAuth 2.0 credentials.\nStep 1: Create an OAuth Client ID Go to Credentials ‚Üí Create Credentials ‚Üí OAuth client ID.\nApplication type: Web application Authorized redirect URIs: This is the address where Google will return the token after successful authentication. (This value will be obtained from the Amazon Cognito Domain in the next section.) Screenshot:\nFigure 5.2.2: Creating OAuth Client ID and Client Secret.\nNote: Make sure to save the Client ID and Client Secret for later use in the Cognito configuration.\n2. Configuring Amazon Cognito User Pool After obtaining credentials from Google, proceed to AWS Console to configure a User Pool.\nStep 1: Create User Pool \u0026amp; Identity Provider In the Amazon Cognito interface, create a new User Pool. Under Sign-in experience, select Federated identity providers and choose Google.\nFill in the Client ID and Client Secret obtained from Google Cloud.\nScreenshot:\nFigure 5.2.3: Entering Google authentication details into Cognito.\nStep 2: Configure App Client \u0026amp; Domain Under App integration:\nDomain: Create a Cognito Domain. This domain will be used as the Authorized redirect URI back in Google Cloud. App Client Settings: Allowed callback URLs: Your application‚Äôs frontend URL. OAuth 2.0 Grant Types: Select Authorization code grant. OpenID Connect scopes: Select email, openid, profile. Screenshot:\nFigure 5.2.5: Configuring Redirect URL and OAuth Scopes.\n3. Testing the Configuration (Hosted UI) To verify your configuration, open the Cognito-hosted Hosted UI.\nIf the \u0026ldquo;Continue with Google\u0026rdquo; button appears and functions correctly, the setup is successful.\nScreenshot:\nFigure 5.2.6: Login interface with Google successfully integrated.\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/5-workshop/5.5-frontend/5.5.1-api/","title":"Create API Gateway &amp; Authentication","tags":[],"description":"","content":" üõ°Ô∏è Goal: Create an HTTP API to consolidate multiple Lambda functions into a single endpoint and secure it using a Cognito Authorizer.\nStep 1: Create HTTP API Go to AWS Console \u0026gt; API Gateway. Choose HTTP API (low cost, high performance) \u0026gt; Click Build. API name: AuroraAPI. Click Next and leave the Integrations blank (we will add them later). Stage: Keep the default $default (Auto-deploy). Click Create. Illustration: Step 2: Connect Lambda (Integrations) We need to declare which Lambda functions this API will point to (created in section 5.4).\nGo to Integrations \u0026gt; Manage integrations \u0026gt; Create. Choose Lambda function. Select the function auroratimeEvent (or other functions you created). Repeat for other functions (auroratimeTodo, etc.). Illustration: Step 3: Create Routes Go to Routes \u0026gt; Create. Define API paths: POST /events -\u0026gt; Select integration auroratimeEvent GET /events -\u0026gt; Select integration auroratimeEvent POST /todos -\u0026gt; Select integration auroratimeTodo ‚Ä¶ (Note: Action logic can be handled inside the Lambda code or further divided by more detailed routes.) Illustration: Step 4: Configure CORS (Important) To allow the Frontend (Amplify) to call the API, enable CORS:\nGo to CORS. Access-Control-Allow-Origin: *. Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS. Access-Control-Allow-Headers: Content-Type, Authorization. Click Save. Illustration: Step 5: Configure Authentication (JWT Authorizer) This step secures the API. Only requests with a Cognito token are allowed.\nGo to Authorization \u0026gt; Manage authorizers tab \u0026gt; Create. Authorizer type: JWT. Name: CognitoAuth. Identity source: $request.header.Authorization. Issuer URL: https://cognito-idp.ap-southeast-1.amazonaws.com/ap-southeast-1_TryyHPjm0 (replace with your User Pool ID). Audience: 5dct7sk93a0unassp7komfpidq Click Create. Attach Authorizer: Go back to the Attach authorizers to routes tab, select routes (/events, /todos, ‚Ä¶) and assign CognitoAuth to them. Illustration: "},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/5-workshop/5.4-backend-logic/5.4.2-todo/","title":"CRUD Todo (Tasks)","tags":[],"description":"","content":" üìÖ Function: This function handles all Create, Read, Update, and Delete (CRUD) operations for the user\u0026rsquo;s task list.\nStep 1: Create Lambda Function Function name: auroratimeTodo Runtime: Node.js 18.x Description: API that handles CRUD operations for the Todo table. Image: Step 2: Configure IAM Role (Full Access to Todo) We need to grant full CloudWatch and DynamoDB access to the todo table.\nJSON Policy (CloudWatch Logs):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:ap-southeast-1:080563425614:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogTodo\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:ap-southeast-1:080563425614:log-group:/aws/lambda/auroraTimeTodo:*\u0026#34; ] } ] } ```json ** JSON Policy (CRUD permissions for DynamoDB todo table) { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCRUDOnTodoTable\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:ap-southeast-1:080563425614:table/todo\u0026#34; } ] } Step 3: Implement the Logic (Node.js) Return to the Lambda Function interface and begin writing your Node.js code to handle the CRUD operations.\nH√¨nh ·∫£nh: After completing, click Deploy to save your changes.\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/5-workshop/5.4-backend-logic/5.4.1-event/","title":"Event CRUD (Event Handler)","tags":[],"description":"","content":" üìÖ Function: This function handles all Create, Read, Update, and Delete (CRUD) operations for the user\u0026rsquo;s schedules/events.\nStep 1: Create Lambda Function Function Name: auroraTimeEvent Runtime: Node.js 18.x Description: API for handling CRUD operations for the Events table. Image: Step 2: Configure IAM Role (Full Access to Events) We need to grant full read/write permissions on the events table.\nJSON Policy (CloudWatch Logs permissions):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:ap-southeast-1:080563425614:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:ap-southeast-1:080563425614:log-group:/aws/lambda/auroraTimeEvent:*\u0026#34; ] } ] } ** JSON Policy (CRUD permissions for DynamoDB events table) { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCRUDOnEventsTable\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:ap-southeast-1:080563425614:table/events\u0026#34; } ] } Step 3: Processing code (Node.js) Back to the Lambda Function interface, we will write Node.js code to process CRUD operations.\nH√¨nh ·∫£nh: After completing, click Deploy to save.\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/5-workshop/5.4-backend-logic/5.4.4-resend/","title":"Resend &amp; Route 53 Configuration","tags":[],"description":"","content":" üìß Goal: Before writing the email-sending logic, we need to verify our domain (Domain Verification) to ensure outgoing emails do not land in Spam. We will connect Resend with AWS Route 53.\nStep 1: Add Domain to Resend Go to the Resend Dashboard. Click Add Domain. Enter your domain: auroratime.click. Select Region. Click Add. Resend will provide you with three types of DNS records (MX, SPF, DKIM). Illustration: Step 2: Configure DNS in AWS Route 53 We need to copy the DNS records from Resend and add them to Route 53.\nGo to AWS Console \u0026gt; Route 53 \u0026gt; Hosted zones. Select your domain. Click Create record. Create the MX record (Mail Exchange): Record name: (Leave empty or use the value provided by Resend) Record type: MX Value: Copy from Resend Create the TXT records (SPF \u0026amp; DKIM): Do the same for each TXT record required by Resend. Note: If the Record name ends with your domain, in Route 53 you only need to enter the prefix (e.g., bounces), because Route 53 will automatically append the domain. Illustration: Step 3: Verify and Get API Key Return to Resend and click Verify DNS Records. Wait around 1‚Äì5 minutes until the status turns Verified (Green). Go to API Keys \u0026gt; Create API Key. Name your key and select Sending access. Copy and store this API Key securely Image:\nImage: Illustration: üí° Tip: This DNS configuration improves your domain\u0026rsquo;s reputation, ensuring that system emails from Aurora land in the Inbox instead of Spam.\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/5-workshop/5.1-architecture/","title":"System Architecture &amp; Authentication Flow","tags":[],"description":"","content":"1. High-Level System Architecture The Aurora system is built entirely on a Serverless architecture on AWS, optimizing both scalability and operational cost.\nIt integrates with Google Cloud Platform to provide a seamless Single Sign-On (SSO) experience via Google authentication.\nMain Components: Frontend (Client):\nA Web App (SPA) where users interact with the system, view schedules, and create tasks.\nAuthentication Layer:\nGoogle Cloud Project: Provides OAuth 2.0 Client ID/Secret to authenticate Gmail users. Amazon Cognito: Acts as the federated Identity Provider (IdP), managing the User Pool and issuing temporary AWS credentials to the Frontend. Backend Logic (Compute):\nAWS Lambda: Hosts business logic functions (Create Event, Update Tasks, Trigger Email Notifications). Database:\nAmazon DynamoDB: Stores Events and Daily Worklogs.\nUses UserId as the Partition Key to isolate each user\u0026rsquo;s data securely. Notification Service:\nEmail Sending Logic: Triggered by Lambda when a new event is created or when the scheduled time arrives.\nSends HTML-formatted emails using SES or a custom Email API. 2. Authentication Flow This flow ensures that only authenticated users can access their personal data.\nAurora uses Cognito Federated Identities combined with Google OAuth 2.0.\nStep-by-step Process: User Login:\nThe user clicks ‚ÄúSign in with Google‚Äù on the Frontend.\nGoogle OAuth:\nThe user is redirected to Google‚Äôs login page.\nAfter a successful login, Google returns an Id Token (JWT).\nToken Exchange:\nThe Frontend sends the Id Token to Amazon Cognito.\nVerification \u0026amp; Session Handling:\nCognito validates the Token with Google. If the token is valid:\nCognito creates/updates the corresponding user profile in the User Pool. Cognito returns temporary AWS credentials (Access Key, Secret Key, Session Token) to the Frontend. Authorized Requests:\nThe Frontend uses these credentials to access API resources (via API Gateway or directly invoking Lambda/DynamoDB through AWS SDK) with permissions defined in IAM Roles.\n3. Data Flow: Creating an Event \u0026amp; Sending Email Notifications When a user creates a new event (e.g., ‚ÄúTeam Meeting at 9:00 AM‚Äù), the data flow proceeds as follows:\nFrontend ‚Üí API Request:\nThe frontend sends a POST request containing the event details to the API endpoint.\nAWS Lambda Trigger:\nThe Lambda function is invoked and performs:\nInput validation Writing event data to DynamoDB (Table: AuroraEvents) Email Notification Trigger:\nAfter writing to DynamoDB, Lambda triggers the email-sending logic. The system generates an HTML email body. The Email Service (SES/Gmail API/Resend) sends the message to the user‚Äôs inbox. üí° Highlight: Integrating Google Login eliminates the need for users to manage an additional password, while benefiting from Google‚Äôs built-in two-factor authentication for enhanced security.\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/4-eventparticipated/4.1-event1/","title":"AWS Cloud Day Vietnam - AI Edition 2025","tags":[],"description":"","content":"AWS Cloud Day Vietnam - AI Edition 2025 - Date: September 18, 2025 - Location: 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nEvent Overview A pivotal gathering for Vietnam\u0026rsquo;s tech and business communities, focused on accelerating digital transformation through the convergence of Cloud Computing and Artificial Intelligence.\nKey Objectives:\nDemocratize Generative AI: Move GenAI from concept to practical, context-aware enterprise applications. Align Business \u0026amp; IT: Bridge the gap between business goals and IT, especially in Financial Services. Accelerate Modernization: Provide industry-specific roadmaps for migration and cloud-native development. Strengthen Security: Promote a \u0026ldquo;security by design\u0026rdquo; mindset across the application lifecycle. Key Takeaways \u0026amp; Learnings Data is the Differentiator: A comprehensive data strategy is a prerequisite for successful Generative AI. Modernization is a Continuous Journey: The goal is not just to migrate but to \u0026ldquo;Migrate to Operate\u0026rdquo; and innovate continuously. Business-Led Technology: Technology initiatives must be driven by clear business outcomes. Security is Everyone\u0026rsquo;s Responsibility: Security must be integrated from the first line of code. Application to Work Audit Data Readiness: Assess our current data strategy to ensure it can support future GenAI initiatives. Pilot GenAI in DevOps: Experiment with AI-driven code generation and automated testing to improve development velocity. Benchmark Modernization Efforts: Analyze case studies from Honda Vietnam (SAP migration) and Masterise Group (VMware migration) to refine our own modernization roadmaps. Implement \u0026ldquo;Security at Scale\u0026rdquo;: Integrate security tools and best practices throughout the entire development lifecycle. Event Photos "},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Unlock generative AI capabilities with DataRobot from AWS Marketplace by Shun Mao, Luke Shulman, and Nathaniel Daly on 20 DEC 2024 in Amazon Bedrock, Amazon Machine Learning, Amazon SageMaker, Amazon SageMaker Autopilot, Amazon Titan, AWS Marketplace, Generative AI, Intermediate (200) Permalink Share Generative AI has been a central topic in the technology space since late 2022. However, with the introduction of many more new large language models (LLMs) and the identification of issues such as hallucination, limited context window length, prompt engineering, and cost, the generative AI community has realized that more features and functionalities should be added to use LLMs safely and efficiently in an enterprise environment.\nThese combined complexities have made it challenging for enterprises and organizations to quickly embrace generative AI. Such challenges include lacking generative AI expertise and staff, having no framework to evaluate different models, lacking model deployment mechanisms, not knowing how to implement guardrails and monitoring for LLMs or how to combine traditional predictive AI with generative AI, and more. Overall, enterprises and organizations need a consolidated tool that can tackle these challenges while requiring minimal AI expertise and an affordable budget.\nDataRobot, an AWS Partner and AWS Marketplace seller, provides a solution to address these challenges. DataRobot is a complete AI lifecycle platform that offers both predictive and generative AI capabilities with a low-code no-code (LCNC) design to help enterprises and organizations deliver business value and drive innovation with AI at a faster pace. DataRobot has achieved AWS Competencies in ML, data and analytics, and financial services, and holds the AWS Service Ready Specialization for Amazon SageMaker. DataRobot offers various deployment types, including multi-tenant software as a service (SaaS) built on AWS, single-tenant SaaS, and Amazon Virtual Cloud (Amazon VPC) deployment to meet the requirements of different companies and industries.\nThis post is going to introduce the overall architecture of DataRobot AI Platform and also demonstrate how you can build a GenAI application, from LLM selection, prompt testing and model evaluation to Retrieval Augmented Generation (RAG), LLM monitoring and guardrails. Everything has been combined into one unified DataRobot AI platform with a streamlined user experience.\nSolution overview DataRobot‚Äôs SaaS platform is built entirely on AWS with a modernized Kubernetes design, providing customers with robust, scalable, and trustworthy AI solutions. AI projects start with data ingestion, where DataRobot provides secure integrations to a wide selection of data sources, such as Amazon S3, Amazon Athena, and Amazon Redshift, as well as data stores from other providers like Snowflake. The following diagram shows the high-level architecture of the solution.\nOverall architecture diagram of DataRobot‚Äôs SaaS Figure 1: Overall architecture diagram of DataRobot‚Äôs SaaS\nFor predictive AI, you can use DataRobot to store, view, transform, and analyze your data with minimal data science knowledge in just a few clicks. These capabilities are backed by a powerful data processing engine. After data preparation, DataRobot uses its expertise in AutoML, which runs parallel ML training jobs on multiple selected high-quality ML models and all necessary feature processing steps automatically. Trained models are ranked in an integrated leaderboard with detailed information about each model, such as accuracy, feature importance map, receiver operating characteristic (ROC) curve, processing recipes, and prediction explanations. You can deploy your models within DataRobot or to other platforms, such as Amazon SageMaker or Snowflake, within minutes after selecting the best model.\nFor generative AI, DataRobot provides an LLM playground, a space where you can create and interact with LLM blueprints using different leading LLMs, including Anthropic‚Äôs Claude in Amazon Bedrock, and Amazon Titan models. The playground has integrated preselected vector databases to let customers to build chat or RAG applications without deep ML expertise. Different LLM blueprints can be created and compared side by side for prompt testing. Various preselected LLM evaluation metrics and custom metrics can be chosen and deployed to monitor the performance of LLMs, which is crucial for enterprise and organizational LLM adoption. After testing the LLM blueprint and deciding on the prompt and metrics, customers can move it into deployment for production.\nFor both predictive and generative AI, DataRobot equips the models with monitoring tools that are quick to set up. The platform can monitor many preselected metrics as well as custom-designed metrics, such as service health, latency, token size, error rate, and cost. For generative AI, DataRobot provides guardrails to help prevent prompt injection, sentiment and toxicity classification, personal identifiable information (PII) detection, and more.\nIn the following sections, we demonstrate the major steps on how you can easily build a predictive and generative AI application in DataRobot.\nIn the walkthrough, you learn how to:\nConnect to data in Amazon S3 and create a data wrangling recipe to prepare the data Build a predictive model within DataRobot and deploy it to Amazon SageMaker Create an LLM blueprint that combines Anthropic‚Äôs Claude in Amazon Bedrock with grounding data Evaluate and optimize the blueprint against metrics such as Recall-Oriented Understudy for Gisting Evaluation (ROUGE), faithfulness, and confidence Package the LLM blueprint with guardrails to maintain safety and performance Launch an AI app that can predict NBO and automatically generate email outreach Prerequisites Before getting started, make sure you have the following prerequisites:\nAn AWS account with access to Amazon Bedrock models A DataRobot account. DataRobot can be purchased through AWS Marketplace. You can also visit the DataRobot site to request a free trial. Solution walkthrough: Build a predictive and generative AI application powered by Anthropic‚Äôs Claude models in DataRobot The use case we demonstrate here is to create a next best offer (NBO) email campaign where the goal is to proactively reach out to customers likely to churn and provide them with an offer that will help retain them as customers.\nWe use a predictive model to recommend the NBO for each customer and then use generative AI to customize the email for each customer.\nStep 1: Connect the data source in Amazon S3 In DataRobot Workbench UI, we can start a new Use Case and add NBO data by connecting to an Amazon S3 bucket. The NBO data is prepared in-house based on the public IBM telco customer churn dataset. Make changes to this dataset by adding another text-based column named ‚Äúcustomer plan‚Äù and changing the prediction target from Churn Value to Next Best Offer. This change turns the prediction to be a multi-class classification problem. This was derived by applying some common-sense business rules to the reason codes from the original data. Meanwhile, we can create new columns to track how long customers have been on the phone with customer service, providing a key feature for our prediction. Use DataRobot‚Äôs data wrangler to create a wrangling recipe with a sample of data and then apply it to the full dataset. Add input data from Amazon S3 in DataRobot UI Figure 2: Add input data from Amazon S3 in DataRobot UI\nStep 2: Create a predictive model for the NBO Once the data preparation is done, we can train a predictive model to predict which type of offer is best suited for a customer in order to avoid churning. To do that, we follow these steps:\nIn DataRobot Workbench UI, start an experiment for model training. For the multiclass type, select Offers as the prediction target. Run Autopilot Register to train a few ML models. Training will take place automatically with preprocessing steps designed uniquely for each different model. All the processing recipes can be viewed in the UI, shown as blueprint, so customers can view what‚Äôs really happening before the actual model training. This is shown in the following screenshot. Blueprint for modeling and processing recipes Figure 3: Blueprint for modeling and processing recipes\nThese recipes can finish in minutes given the small size of the tested datasets. When you deploy the model, you can directly deploy it within DataRobot without worrying about the underlining infrastructure. You also have the option to deploy it into Amazon SageMaker with one-click deployment, as shown in the following screenshot. This provides great flexibility for choosing deployment infrastructure.\nSageMaker one-click model deployment within DataRobot] Figure 4: SageMaker one-click model deployment within DataRobot\nStep 3: Create the LLM blueprint Once we have finished deploying the predictive model, we can build the generative AI part of the application, which is a RAG setup. Several market leading LLMs are already available in the DataRobot‚Äôs LLM playground, such as Anthropic‚Äôs Claude from Amazon Bedrock and the Amazon Titan model, as well as models from other providers, giving customers a great choice in selecting the right model for their applications.\nNavigate to the LLM playground from DataRobot Workbench and create an LLM blueprint by picking the Claude 3 model as the LLM from the drop-down list. After the model has been selected, several LLM related parameters can be adjusted to tune the results, such as Max completion tokens, Temperature and Token selection probability cutoff. On the same page, vector databases and system prompts can be configured to work with the LLM. At this time, users can try different combinations of LLM, chunking strategy, and prompting strategy to derive an optimal result. Users can build multiple blueprints and compare them against different metrics such as ROUGE, confidence, and faithfulness. The data we used to send to the vector database is a collection of five internal plan documents discussing various details of the cell phone plans. The following screenshot shows the LLM playground UI.\nDataRobot LLM playground UI Figure 5: DataRobot LLM playground UI\nStep 4: Set up LLM guardrails Once the LLM blueprint has been created and tested, choose the LLM blueprint actions and then choose Send to model workshop. In the model workshop, adjust the runtime environment variables as needed. The Evaluation and moderation section contains metrics that you can pick to evaluate the model performance and safety over time. DataRobot provides some of the ready-to-use metrics to get started, such as PII detection, sentiment analysis, toxicity, and more. In addition, you can design your own custom metrics based on your unique business logic. After you complete all the configurations, you can test the model or create new versions. DataRobot recommends testing it before deployment. Next, choose Register model, provide the registered model or version details, and then choose Register model again to add the LLM to the Registry. The registered model version opens in the Model directory, which contains the Deploy button for users to quickly deploy the model. DataRobot model workshop UI Figure 6: DataRobot model workshop UI\nLLM evaluation metrics available in DataRobot Figure 7: LLM evaluation metrics available in DataRobot\nStep 5: LLM deployment options As in the predictive model, you can deploy your LLM models within DataRobot or into other third-party platforms, such as Amazon SageMaker or Amazon Elastic Kubernetes Service (Amazon EKS). Here we use DataRobot one-click deployment to deploy the model to Amazon SageMaker. Once deployed, we can monitor the performance of both the generative and predictive portions of the app. You can create custom metrics like latency, return on investment (ROI), or readability, which can be viewed under the same pane of glass, no matter whether those models are built in DataRobot, on AWS, or elsewhere. This is shown in the following screenshot.\nMetrics monitoring dashboard Figure 8: Metrics monitoring dashboard\nStep 6: Host application in DataRobot With the predictive and generative models deployed, we can build a web application by utilizing both models and host the application within DataRobot. DataRobot supports hosting custom applications in a variety of frameworks, such as Streamlit, Shiny and Flask. For detailed information on hosting custom applications in DataRobot, please refer to the Create custom applications article in the DataRobot documentation. Here, as an example we use python to build a simple web application with Streamlit. The interface of the application is shown in figure 9. The application picks a customer by name and predicts whether the customer will need a certain type of offer or not to avoid churn. The result of this part is derived from the response of the predictive model we deployed. If it determines the customer needs an offer, the app will leverage the generative model we deployed previously to generate an example email campaign with a purposely curated offer based on the customer‚Äôs unique situation. After that, the marketing team can send the email to the customer.\nExample application that combines both predictive and generative AI Figure 9: Example application that combines both predictive and generative AI\nCleanup To clean up the resources created in this walk-through, follow these steps:\nOn the AWS console under the SageMaker panel, remove the SageMaker deployments by deleting the SageMaker endpoints and endpoint configurations. In the DataRobot UI, remove all DataRobot deployments. There are no additional consumption costs for the DataRobot LLM playground or predictive experiments.\nConclusion DataRobot is a comprehensive AI platform for teams to derive values from AI at faster speed. For companies and organizations of different sizes, DataRobot can help you build and deliver end-to-end generative and predictive AI solutions for your business, offering choice, flexibility, and top-of-the-line components. With the collaboration and tight integrations between DataRobot and AWS, customers can also build their applications by combining the capabilities of the two. To get started with DataRobot, visit\nAWS Marketplace. To learn more examples of how DataRobot can help you in the generative AI space, refer to the DataRobot GenAI Delivered demos.\nAbout Authors\nShun Mao Shun Mao is a senior partner solutions architect in the artificial intelligence and machine learning (AI/ML) independent software vendor (ISV) partner team at Amazon Web Services. He has years of experience in data science, analytics, AI, and cloud across different industries, including oil and gas and pharmaceuticals. At AWS, he is helping strategic AI/ML partners to build novel products and solutions to bring business value to customers. Outside of work, he enjoys fishing, traveling, and playing ping-pong.\nNathaniel Daly Nathaniel Daly is a principal product manager at DataRobot focusing on the development of predictive and generative AI solutions. He‚Äôs focused on bringing advances in data science to users such that they can use this value to solve real world business problems.\nLuke Shulman Luke Shulman is a lead data scientist at DataRobot. Luke has over 12 years of experience in data analytics and data science. Prior to joining DataRobot, Luke led implementations and was a director on the product management team at Arcadia.io, the leading healthcare SaaS analytics platform and at N1Health. At DataRobot, Luke is working with a number of Fortune 100 companies using DataRobot. He is also an active contributor to the DrX extensions to the DataRobot API client focusing on model evaluation and MLFlow integration. An avid champion of data science, Luke has also contributed to projects across the data science ecosystem including Bokeh, Altair, and Zebras.js.\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Unhoused individuals gain shelter, prove their identity using AWS-powered solution Kiip by Noah Harlan and Jessie Metcalf on 03 NOV 2023 in Amazon Bedrock, Amazon Chime, Amazon Cognito, Amazon Simple Storage Service (S3), AWS Key Management Service, AWS Lambda, AWS Marketplace, Customer Solutions, Nonprofit, Public Sector, State or Local Government Permalink Share Noah Harlan stands alongside three Kiip employees at an outdoor event where they are informing others about their solution Noah Harlan, left, and Kiip colleagues at a Homeless Connect Day event in Los Angeles.\nAccess to key documentation is both a struggle and a necessity for vulnerable and at-risk communities. For the unhoused, being ‚Äòdocument ready‚Äô is essential to avoid delays or denial of access to benefits and services they need. For one individual in Los Angeles recently, it meant the difference between having a safe bed in a shelter and being in police custody.\nThe issue of document access and related administrative burdens is so ubiquitous that public health researchers have published papers about the detrimental impact a lack of identification has on those experiencing homelessness. Social Security cards, birth certificates, and proof of disability are crucial for accessing services and applying for housing. Without proper documentation, unhoused individuals face overwhelming barriers to stability and opportunity. But new technologies and tools address these problems while helping the organizations who serve vulnerable populations. One innovative solution called Kiip, powered by Amazon Web Services (AWS), takes a unique approach to this problem by empowering individuals with access and control over their own personal, vital documents.\nEmpowering individuals with ownership of their own data\nStarting in July 2023, hundreds of staff at the Homeless Outreach Program Integrated Care System (HOPICS) began using Kiip to store documents, complete forms and applications, and streamline communications with incoming customers. They aimed to provide every person coming through their doors access to their own documents no matter where they went. As one client discovered, you never know when a key document is needed at a critical moment.\nThis past August, police detained a resident of a HOPICS residential shelter as a suspect in a crime. Law enforcement knew the name of the suspect and this individual matched a description but did not have any identification. Unable to prove they were not the perpetrator, police prepared to take the individual into custody. About to be handcuffed, the individual remembered their Kiip account. They explained to the officers that HOPICS set them up with an account that saved all of their identifying documents when they first entered the shelter.\nOpening their phone, they were able to provide copies of their ID, Social Security card, and even a fully signed copy of their intake forms with the HOPICS case manager who set them up with Kiip. With their identity verified, police released the individual.\nThis interaction seems minor but for someone in a vulnerable community, it is potentially life-altering. If this individual remained in custody, they could have lost their bed in the shelter, requiring a restart of their journey towards being permanently housed.\n‚ÄúWhen we created Kiip, we believed that helping people have access to their own information would not only improve the process of accessing benefits and services but would help them in a host of other needs beyond any one interaction,‚Äù said Noah Harlan, Founder and Chief Executive Officer of Kiip. ‚ÄúWe built Kiip to help individuals overcome administrative burdens faced when obtaining benefits and services, but also to reduce the punishing cognitive burdens placed on at-risk and vulnerable communities. We‚Äôre thrilled that it‚Äôs already helped hundreds of individuals apply for benefits, and now learning that someone was able to help themselves in a potentially dire situation unrelated to their applications was proof that this model works for individuals and communities.‚Äù\nUsing technology to improve social services\nHOPICS is one of five housing coordinators for Los Angeles County with a focus on South Los Angeles. With nearly 500 staff, HOPICS provides a broad range of services including shelters for individuals and families, access centers, outreach teams, and community navigation teams providing housing, behavioral health, and reentry support throughout the community. HOPICS was founded by Mike Neely 35 years ago, when he was a homeless veteran sleeping on Skid Row. Neely applied his lived experience to become a community organizer, eventually launching a pilot program with a $50,000 block grant. From that start, HOPICS has helped thousands of individuals.\n‚ÄúOur partnership with Kiip is changing the way people in Los Angeles engage with the homeless response system by empowering individuals with a resource to store their most vital documents securely,‚Äù said Ben Kay, Associate Director of HOPICS for Access and Triage.\nKiip is a cloud-native software-as-a-service (SaaS) platform that leverages a host of AWS‚Äôs industry-leading features including serverless computing via AWS Lambda, scalable secure storage with Amazon Simple Storage Service (Amazon S3) and AWS Key Management Service (AWS KMS), and Amazon Cognito for identity management. Kiip is a HIPAA-compliant, scalable solution that ensures every feature is available whenever and wherever needed. Kiip uses some of AWS‚Äôs latest and most sophisticated tools, including Amazon Chime for secure multi-party chat and Amazon Bedrock AI tools for private, secure large language model (LLM) document introspection and categorization.\nKiip, working with HOPICS and powered by AWS, enables unhoused individuals to securely keep, manage, and share copies of their vital documents. The solution also provides community-based organizations, agencies, and other service providers secure multi-party chat, document exchange, form filling, and legally binding e-signatures and referrals. Kiip‚Äôs core value proposition for its service provider customers is to improve the application process for their services. It uses a ‚Äúproblem solving‚Äù modality that pushes information, access, and control back to the individuals themselves by making needed documents accessible and shareable. This inversion of traditional portal software empowers individuals beyond the relationship with just their service provider.\nSince launching in July 2023 with HOPICS, Kiip has served more than 800 clients and securely stored more than 3,500 vital documents‚Äîincluding more than 1,000 applications.\nThe broader impact\nKiip is revolutionizing how a community can approach homelessness and its associated challenges. It empowers individuals, improves the efficiency of service providers, and provides a significant contribution in the battle to end homelessness. When facing the complex issue of homelessness, solutions like Kiip provide hope and practical support to those in need. With organizations like HOPICS leading the way, there are new tools that empower a better future where homelessness is rare and temporary.\nAvailable on AWS Marketplace\nPublic sector organizations need to balance extensive regulatory and compliance obligations with cost-effective procurement strategies. AWS Marketplace helps simplify and streamline procurement for public sector organizations, enabling them to spend their budgets driving their missions forward rather than on administrative overhead. Learn more about Kiip on AWS Marketplace and read more blog posts about how the cloud helps communities prevent and combat housing instability.\nAre you a nonprofit that wants to learn more about how cloud computing can accelerate your mission? Do you have questions about how AWS can help your government agency provide innovative services for constituents? Reach out to the AWS Public Sector team.\nRead related stories on the AWS Public Sector Blog:\nDelivering homeowner assistance funds quickly to people in need with the cloud Helping local governments address the housing crisis with the cloud Using the cloud to get rental assistance quickly to those in need Using the cloud to integrate homelessness data with whole-person care services TAGS: AWS for nonprofits, AWS Public Sector, customer story, homelessness, state and local government Noah Harlan Noah Harlan Noah Harlan is the Founder and CEO of Kiip, PBC. Noah has nearly two decades of experience building and leading digital organizations. Noah was Co-Founder and Head of Product for Breadcrumb POS (sold to Groupon in 2012) and co-founded and led Two Bulls, a leading global digital product consultancy (sold to the Dept/Carlyle Group in 2022). Noah was also Director of EdgeX Foundry and the AllSeen Alliance, two open-source consortia under the Linux Foundation umbrella, the former of which he co-founded and the later of which he served as president. Noah is in his second term as an elected member of the Community Education Council for District 1 in New York City. Noah holds a Computer Science degree from Williams College and received an Emmy Award for his work with Advanced Digital Interactivity.\nJessie Metcalf Jessie Metcalf Jessie Metcalf leads data and digital transformation engagements for the Amazon Web Services (AWS) state and local government team. In this role, she works with leaders who want more from the data they have ‚Äì leaders who are curious and determined to understand how they‚Äôre actually impacting all of us with which programs so that they can focus on what improves that experience.\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Databricks modernizes healthcare data on AWS by Venkat Viswanathan, Amandip Kaler, Harrison Holstein, and Vaishali Gohel on 13 MAY 2025 in Amazon AppFlow, Amazon Bedrock, Amazon EC2, Amazon Kinesis, Amazon Nova, Amazon Simple Storage Service (S3), Amazon Titan, AWS Database Migration Service, AWS Glue, AWS Identity and Access Management (IAM), AWS Key Management Service, AWS Marketplace, AWS PrivateLink, AWS Trainium, Graviton Permalink Comments Share This post is co-written with Rebecca Nissan, Sr. Delivery Solutions Engineer, Healthcare and Life Sciences, Field Engineering, Databricks.\nIn today‚Äôs healthcare data-centric landscape, organizations face challenges (such as disparate data silos) across their electronic health records (EHRs), medical imaging, and other on-premises data warehouses. Healthcare data is growing at such a rapid rate that one study observed the healthcare industry is growing faster than other verticals and contributes approximately 30 percent of the world‚Äôs data volume today.\nAs a result, organizations need a secure and easy-to-use data platform in order to centralize and utilize the data efficiently. Databricks on Amazon Web Services (AWS) offers a unified data and analytics platform that breaks down these data silos and integrates the data engineering capabilities of Databricks with the secure and reliable infrastructure of AWS.\nWe will walk through how Databricks and AWS, together, can help address large data silos to transform personalized patient care through generative AI and machine learning.\nIndustry challenges driving cloud data platform adoption The healthcare industry is rapidly evolving in how it manages and utilizes data. The industry is increasingly adopting modern data architectures in order to keep up with the quickly changing landscape.\nLet‚Äôs explore some of healthcare‚Äôs current data challenges:\nDigital transformation in healthcare is driven by a shift away from fee-for-service models to value-based care (VBC) and outcome-based payments. Patient medical history, medical imaging, wearable health information, genomics, and other medical research data play a key role in driving better outcomes for patients. However, the data needs for executing on VBC strategies surpass the capabilities of legacy technologies and require modernization to meet the demands of the business. On-premises data silos result in managing petabytes of data across multiple segments of the organization, which may be locked in different healthcare data modalities. Personalized engagement requires the centralizing of a patient‚Äôs data in order to allow for precision medicine based on a patient‚Äôs health history and requirements. AI-powered innovations with slow adoption rates hinders the ability to leverage more advanced automation of clinical documentation, and enhanced diagnostic accuracy through AI-assisted imaging. The adoption of generative AI for administrative tasks and patient risk predictions would also benefit patient care. These are driving factors for healthcare customers beginning to adopt modern data strategies in the cloud. The scale and speed of innovation in the cloud enables customers to scale resources for peak times when needed and down-scale when not needed, leading to cost efficiency in storage and compute. Consolidation of data silos also leads to a reduction in the duplication of data‚Äîproviding a single source of truth.\nDatabricks for healthcare In today‚Äôs data-driven healthcare landscape, organizations need robust, secure, and scalable solutions to transform vast amounts of healthcare data into actionable insights. The Data Intelligence Platform from Databricks revolutionizes how organizations can use their data to improve patient outcomes.\nDatabricks serves as a unified open analytics platform for building complete data use cases on a data lakehouse, a term that combines data warehousing and data lakes under one technology. Founded by the creators of Apache Spark, this architecture leverages the scalable nature of cloud technology for storage, data processing, and AI while adding the structure, data governance, and business intelligence experience of traditional data warehouses.\nAWS collaborates with Databricks to bring the powerful data lakehouse platform of Databricks together with the comprehensive cloud infrastructure of AWS. This collaboration provides healthcare organizations with a secure, scalable, and compliant environment for their critical data workloads‚Äîenabling innovation to drive improvements to patient care.\nWhy Databricks on AWS Figure 1: High level architecture of data sources and outcomes of Databricks on AWS\nFigure 1: High level architecture of data sources and outcomes of Databricks on AWS\nDatabricks on AWS provides a platform to unify the diverse ecosystem of data silos including all EHR data, medical imaging, laboratory systems, revenue cycles, pharmaceuticals, supply chain, clinical decision support, and more. The decision to run Databricks on AWS represents a strategic choice for healthcare organizations that unlocks tremendous value. It can help with predicting disease onset in real time in inpatient settings, and measuring patient safety metrics (like fall fractures). It can facilitate closing gaps of care with preventative screenings (reducing the length of stay and improving care treatment plans), and optimize provider performance in bundled payment arrangements to name a few.\nThere are several compelling technological advantages of Databricks with AWS.\nAvailability and reliability\nThe foundation of these advantages lies in the AWS global infrastructure, offering the highest availability and resiliency in the industry. Databricks on AWS is available in 13 AWS Regions and multiple Availability Zones per Region. This means healthcare organizations can deploy Databricks workloads with confidence, confirming business continuity and meeting stringent regulatory requirements for data residency and disaster recovery.\nGiving customers simplicity and flexibility for generative AI\nWhile Databricks simplifies the architecture with a lakehouse and accelerates speed to value, AWS provides powerful and flexible tooling. These enable healthcare organizations with unparalleled capabilities to build cutting edge research using generative AI, medical imaging, genomics, autonomous medical coding, drug research and discovery, and personalized treatment plans.\nDatabricks Mosaic AI Gateway supports serving models across all clouds, including those available in Amazon Bedrock which provides access to a choice of leading foundation models (FMs) through a single API. This includes Amazon Titan and Amazon Nova models as well as those from leading AI companies like Anthropic, AI21 Labs, Meta and more. Together, Mosaic AI Gateway and Amazon Bedrock give customers the flexibility and choice to utilize the right model for their use-case leveraging the broad selection of models available in Amazon BedRock in addition to the other models supported by Mosaic AI Gateway.\nDatabricks Mosaic AI provides complete connectivity and transparency between data and AI, powered by its native integration with Unity Catalog‚Äôs observability tools which enable improved model accuracy and spending oversight.\nFurthermore, leveraging AWS Trainium and Inferentia chips enables organizations to optimize both training and inference costs while maintaining high performance.\nMaximized return on investment and cost optimization\nHealthcare organizations can leverage AWS Graviton processors, which offer up to 40 percent better price performance compared to traditional x86-based instances. The ability to utilize Amazon EC2 Spot Instances for Databricks workloads can reduce compute costs by up to 90 percent compared to on-demand pricing.\nSpot Instances on AWS provide a two-minute interruption notice before termination, helping customers balance fault tolerance with cost optimization. This cost efficiency extends to data storage through intelligent tiering between Amazon Simple Storage Service (Amazon S3) storage classes and integration with AWS cost management tools. Healthcare organizations can maintain control over their technology spending while scaling their analytics capabilities.\nAdopting Databricks on AWS also accelerates time-to-insights compared to building and maintaining a custom data analytics platform. Custom solutions require dedicated platform engineers and architects to build and support ongoing operational maintenance activities (such as patching, cluster optimization, and managing data ingestion pipelines). With Databricks on AWS, organizations can re-focus their in-house expertise to extracting value from their data rather than maintaining complex data architectures.\nSecurity and compliance\nSecurity and compliance capabilities are paramount for healthcare organizations. Native integration with AWS security services, including AWS Key Management Service (AWS KMS) for encryption key management, AWS PrivateLink for secure network connectivity, and AWS Identity and Access Management (IAM) for granular access control, make it straightforward for healthcare organizations to maintain HIPAA compliance and protect sensitive patient data.\nThe Databricks platform‚Äôs support for the AWS shared responsibility model provides clear delineation of security obligations, streamlining compliance audits and risk management processes.\nFor customers getting started on AWS, we recommend exploring the AWS Landing Zone Accelerator (LZA) for Healthcare. This AWS open-source solution helps you quickly deploy a secure foundation aligned with AWS best practices, specific to the healthcare industry, to set you up for success.\nThe combination of these advantages makes Databricks on AWS a strategic choice for healthcare organizations. Whether implementing precision medicine initiatives, optimizing clinical trials, or improving operational efficiency. The platform provides the technical capabilities, security features, and economic benefits needed to drive healthcare innovation forward.\nUnlock your healthcare data with Databricks on AWS Figure 2: Databricks on AWS for Healthcare reference architecture\nFigure 2: Databricks on AWS for Healthcare reference architecture\nThe Databricks on AWS for Healthcare reference architecture begins with a secure landing zone through the AWS Landing Zone Accelerator for Healthcare. This serves as the foundation for integrating diverse healthcare data sources, including electronic health records, medical imaging, and genomics research.\nData integration is achieved through multiple pathways, including Databricks tools such as Auto Loader, Lakeflow Connect, and Structured Streaming, as well as AWS services like AWS Database Migration Service (AWS DMS), Amazon AppFlow, AWS Glue, and Amazon Kinesis. These services allow for both batch and streaming data ingestion into a data lakehouse.\nThe data is stored in the lakehouse, which is backed by Amazon S3. Medallion architecture from Databricks is utilized for data transformation and curation. Finally, the processed data can be integrated with various services, including Amazon Bedrock, providing API access to industry-leading foundation and large language models.\nOnce you have filtered, cleaned, and augmented both your structured and unstructured data you have production data‚Äîready to be delivered to downstream users and applications. Databricks customers can take advantage of Delta Sharing, an open-source approach to securely share live data from your lakehouse to any computing platform, analytics and AI.\nKey benefits of Delta Sharing include open cross-platform sharing (avoiding vendor lock-in), the ability to share live data without replication, and centralized governance with Unity Catalog. Delta Sharing enables you to integrate data with your preferred business intelligence (BI) and analytics tools (including Amazon QuickSight). From there healthcare customers can build reports, visualizations, and applications that unlock insights from data that was previously locked in data silos.\nSummary The combination of Databricks and AWS represents a powerful solution for healthcare organizations looking to transform their data analytics capabilities. We explored how this partnership delivers the security, scalability, and sophisticated analytics capabilities needed to drive innovation in healthcare while maintaining regulatory compliance and cost efficiency.\nThe future of healthcare analytics is here, and it‚Äôs powered by Databricks on AWS. Join the growing community of healthcare organizations that are leveraging this powerful combination to improve patient outcomes, accelerate research, and unlock your data to drive innovation.\nConnect with Databricks or an AWS Representative for live demos and discuss how we can support your journey to a modern data architecture.\nReady to accelerate your healthcare organization‚Äôs data initiatives? Here are some ways to get started:\nTry a 14-day free trial of Databricks by utilizing the streamlined deployment experience through the AWS Marketplace Browse the Databricks Healthcare \u0026amp; Life Sciences Solution Accelerators to get started quickly on common use-cases Visit AWS for Healthcare \u0026amp; Life Sciences to learn more about healthcare specific solutions TAGS: Databricks Venkat Viswanathan Venkat Viswanathan Venkatavaradhan (Venkat) Viswanathan is a Senior Solutions Architect at Amazon Web Services. Venkat is a Technology Strategy Leader in Data, AI, ML, and Advanced Analytics. Venkat is a Global SME for Databricks and helps AWS customers design, build, secure, and optimize Databricks workloads on AWS.\nAmandip Kaler Amandip Kaler Amandip Kaler is a solutions architect for Healthcare at Amazon Web Services (AWS). He has a degree in computer engineering and supports healthcare organizations to modernize and achieve their missions.\nHarrison Holstein Harrison Holstein Harrison is a Senior Solutions Architect supporting Global AWS Public Sector Independent Software Vendor partners. He has been with AWS for 6 years and enjoys working with partners to deliver joint solutions that help customers modernize their data strategy while maintaining their security and compliance objectives.\nVaishali Gohel Vaishali Gohel Vaishali Gohel is a Solutions Architecture Leader at Amazon with over two decades of experience in transforming healthcare organizations through innovative cloud technology solutions. She\u0026rsquo;s a strategic technology executive specializing in large-scale cloud migrations and advanced data strategies using AI, ML, and generative AI. Vaishali handles consistently delivering solutions that drive business growth, enhance patient care, and improve operational excellence.\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Van Sang\nPhone Number: 0983135379\nEmail: sangnvse183276@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 24/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Get acquainted with FCJ members (mentor, buddy, HR, project team).\u0026ndash; Read and take note the internal rules, code of conduct and communication channels (email, chat, meeting) of FCJ Workforce Program. 08/09/2025 08/09/2025 https://policies.fcjuni.com/ TUE - Learn about AWS and its types of services: Compute, Storage, Networking, Database, Content Delivery, Security, Identity, Analytics (high‚Äëlevel overview).- Learn about AWS Global Infrastructure: Region, Availability Zone, Edge Location, Shared Responsibility Model. 09/09/2025 09/09/2025 WED - Creating Your First AWS Account: understand root user, IAM User, IAM User Group and MFA best practices.000001.awsstudygroup‚Äã- Create AWS Free Tier account (enable MFA, configure basic security and contact information).- Learn about AWS Console \u0026amp; AWS CLI concepts and use cases. 10/09/2025 10/09/2025 https://www.youtube.com/watch?v=waR5S_lljrk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=11 https://www.youtube.com/watch?v=1dG5xutGbr4\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=12 THU - Access Management with AWS IAM: learn IAM users, groups, policies, least‚Äëprivilege principle.000001.awsstudygroup‚Äã- Create Administrator Group and Admin User instead of using root user for daily 11/09/2025 11/09/2025 https://www.youtube.com/watch?v=b9pK1oG534Q\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=13 FRI - Practice basic commands to verify identity and list resources - Managing Costs with AWS Budgets: create a simple cost budget and alert to control Free Tier usage 12/09/2025 12/09/2025 https://www.youtube.com/watch?v=_a09nLVw6Sg\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=15 Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material MON - Networking Essentials with Amazon VPC: review the concepts of VPC, Subnet, Route Table, Internet Gateway, NAT Gateway, Security Group, and NACL (high‚Äëlevel overview). 15/09/2025 15/09/2025 https://000003.awsstudygroup.com/vi/000003.awsstudygroup‚Äã TUE - Practice creating a custom VPC: create a VPC, 1 public subnet, 1 private subnet, attach an Internet Gateway, and configure the Route Table for the public subnet.- Document the CIDR blocks, AZs, and routes used for later EC2 labs. 16/09/2025 16/09/2025 https://000003.awsstudygroup.com/000003. https://www.youtube.com/watch?v=sllYqAECBoM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=56awsstudygroup‚Äã WED - Compute Essentials with Amazon EC2: study the concepts of instance types, AMI, EBS volumes, Security Groups, key pairs, and basic pricing.000004.awsstudygroup‚Äã- Prepare a plan: choose AMI, instance type, subnet, and Security Group for the web server lab. 17/09/2025 17/09/2025 https://www.youtube.com/watch?v=yAR6QRT3N1k\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=74 THU - Attend an AWS event 18/09/2025 18/09/2025 FRI - Attach and configure an additional EBS volume for the EC2 instance (create, attach, format, and mount the volume).- Write a short internal note titled ‚ÄúVPC + EC2 Lab ‚Äì Week 2‚Äù describing the architecture, main steps, and security considerations (CIDR, Security Groups, key pairs). 19/09/2025 19/09/2025 https://www.youtube.com/watch?v=7NjNTnXon5s\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=44youtube‚Äã Week 2 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Week 3 s·∫Ω t·∫≠p trung v√†o RDS + Auto Scaling EC2 + CloudWatch, v√† Friday (T6) ghi r√µ b·∫°n tham gia s·ª± ki·ªán AWS/FCJ.\nWeek 3 ‚Äì Worklog (15‚Äì19/09/2025 pattern ti·∫øp t·ª•c) Day Task Start Date Completion Date Reference Material MON - Database Essentials with Amazon RDS: learn core concepts (DB instance, engine types, storage, Multi‚ÄëAZ, backup, security group for DB).[1] - Review common use cases of RDS for web applications. 22/09/2025 22/09/2025 https://www.youtube.com/watch?v=TQFwQAre0H4\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=59 TUE - Hands-on: create an RDS database instance in the same VPC as the EC2 web server from Week 2.\n- Configure security so that only the EC2 instance (or app subnet) can connect to the RDS instance. 23/09/2025 23/09/2025 https://www.youtube.com/watch?v=SlP-KdSs3IM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=225 WED - Scaling Applications with EC2 Auto Scaling: learn the concepts of Launch Template/Configuration, Auto Scaling Group, scaling policies, health checks.[3] - Design a simple scaling policy based on CPU utilization for the web tier. 24/09/2025 24/09/2025 https://www.youtube.com/watch?v=hFVYG8WqfU0\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=80 THU - Hands-on: create an Auto Scaling Group for web EC2 instances across at least two AZs (if available).\n- Test scale‚Äëout/scale‚Äëin behavior by adjusting thresholds or generating load. 25/09/2025 25/09/2025 https://000006.awsstudygroup.com[3] FRI - Attend an AWS event 26/09/2025 26/09/2025 Week 3 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: D∆∞·ªõi ƒë√¢y l√† Week 4 (kh√¥ng c√≥ event), v·∫´n b√°m theo flow Cloud9 + S3 static website.\nWeek 4 ‚Äì Worklog Day Task Start Date Completion Date Reference Material MON - Set up Hybrid DNS with Route 53 Resolver: learn basic concepts of Route 53, Route 53 Resolver, hybrid DNS between on‚Äëprem and AWS, inbound/outbound endpoints, and Resolver rules (introduction).- Take notes about typical hybrid DNS use cases for enterprise environments. 06/10/2025 06/10/2025 https://www.youtube.com/watch?v=FGicpWOmMDI\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=46youtube‚Äã TUE - Follow the Hybrid DNS lab: create required VPCs, subnets and initial Route 53 configuration for the lab scenario.- Draw a small architecture diagram showing on‚Äëprem DNS, Route 53 Resolver inbound/outbound endpoints, and VPCs. 07/10/2025 07/10/2025 https://000010.awsstudygroup.com/vi/ WED - Configure Security Groups for the Hybrid DNS lab: keep only ICMP (ping) and RDP ports needed for testing, remove unused ports to follow least‚Äëprivilege and improve security.- Test connectivity (ping, RDP) to ensure Security Group rules work as expected. 08/10/2025 08/10/2025 https://www.youtube.com/watch?v=kE_krznNBFU\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=49youtube THU - Complete Hybrid DNS with Route 53 Resolver: configure inbound and outbound Resolver endpoints and create Resolver rules to forward specific domains between on‚Äëprem and AWS.- Test DNS resolution in both directions (from ‚Äúon‚Äëprem‚Äù to AWS and from AWS back to on‚Äëprem). 09/10/2025 09/10/2025 https://www.youtube.com/watch?v=L-2YfZceoAU\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=61000003.awsstudygroup FRI - Set up AWS Transit Gateway: learn concepts and pricing (attachments, data processing) and create a Transit Gateway in the lab account.- Attach existing VPCs to the Transit Gateway and update VPC route tables so traffic between VPCs flows through the TGW; test cross‚ÄëVPC connectivity. 10/10/2025 10/10/2025 https://www.youtube.com/watch?v=W1m_OFPDui0\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=68 https://www.youtube.com/watch?v=QSXgL2KodQI\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i Week 4 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: D∆∞·ªõi ƒë√¢y l√† Week 5 cho module Using AWS Storage Gateway (4 video b·∫°n g·ª≠i), theo ƒë√∫ng format c√°c tu·∫ßn tr∆∞·ªõc. N·ªôi dung t·∫≠p trung v√†o S3 bucket, EC2 host cho Storage Gateway, c·∫•u h√¨nh Storage Gateway v√† test truy c·∫≠p.\nWeek 5 ‚Äì Worklog (AWS Storage Gateway) D∆∞·ªõi ƒë√¢y l√† Week 5 cho module Using AWS Storage Gateway (4 video b·∫°n g·ª≠i), theo ƒë√∫ng format c√°c tu·∫ßn tr∆∞·ªõc. N·ªôi dung t·∫≠p trung v√†o S3 bucket, EC2 host cho Storage Gateway, c·∫•u h√¨nh Storage Gateway v√† test truy c·∫≠p.\nWeek 5 ‚Äì Worklog (AWS Storage Gateway) D∆∞·ªõi ƒë√¢y l√† Week 5 cho module Using AWS Storage Gateway (4 video b·∫°n g·ª≠i), theo ƒë√∫ng format c√°c tu·∫ßn tr∆∞·ªõc. N·ªôi dung t·∫≠p trung v√†o S3 bucket, EC2 host cho Storage Gateway, c·∫•u h√¨nh Storage Gateway v√† test truy c·∫≠p.\nWeek 5 ‚Äì Worklog (AWS Storage Gateway) Day Task Start Date Completion Date Reference Material MON - Using AWS Storage Gateway: review the lab overview and main use cases (file/volume/tape gateway, hybrid storage, backup and archiving).[1] - Understand the overall lab architecture (on‚Äëpremises VM/EC2 acting as gateway, S3 as backend storage). 13/10/2025 13/10/2025 https://www.youtube.com/watch?v=Je2jPk7HhLQ\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=88 TUE ‚Äì Create S3 Bucket‚Äù and create the S3 bucket required for the Storage Gateway lab.[2] - Configure basic settings: bucket name, region, encryption (if required), and relevant tags for the lab. 14/10/2025 14/10/2025 https://www.youtube.com/watch?v=3vSrTeWroSs\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=86[2] WED ‚Äì Create EC2 for Storage Gateway‚Äù and provision the EC2 instance that will run the Storage Gateway.[3] - Configure networking and Security Groups so the EC2 instance can reach S3 and be managed securely (only required ports allowed). 15/10/2025 15/10/2025 https://www.youtube.com/watch?v=xVrhpe8OpVU\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=87 THU ‚Äì Configure AWS Storage Gateway‚Äù (index 88) and activate the gateway using the S3 bucket created earlier.[1] - Configure the appropriate gateway type (for example File or Volume Gateway) according to the lab guide and verify that the gateway status is active. 16/10/2025 16/10/2025 https://www.youtube.com/watch?v=Je2jPk7HhLQ\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=88[1] FRI Test Storage Gateway / Access Data‚Äù (index 89) and perform the lab tests (mount share or volume, read/write files, verify data in S3).[1] - Write a short internal note summarizing ‚ÄúHow AWS Storage Gateway integrates on‚Äëpremises workloads with S3 and when to use it in real projects‚Äù. 17/10/2025 17/10/2025 https://www.youtube.com/watch?v=3Zp9GSMO-VI\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=89 1 2 3\nWeek 5 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material MON - Rest day: no new labs or videos. 20/10/2025 20/10/2025 TUE - Watch ‚ÄúModule 04‚Äë02 ‚Äì Amazon S3 ‚Äì Access Point ‚Äì Storage Class‚Äù.\n- Take notes on S3 Access Points, how they simplify access control for shared datasets, and how storage classes impact cost and performance. 21/10/2025 21/10/2025 https://www.youtube.com/watch?v=_yunukwcAwc\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=105[2] WED - Watch ‚ÄúModule 04‚Äë03 ‚Äì S3 Static Website \u0026amp; CORS ‚Äì Control Access ‚Äì Object Key \u0026amp; Performance ‚Äì Glacier‚Äù.\n- Summarize concepts: S3 static website hosting, CORS configuration, ACL vs IAM/Resource Policies, object key design and performance, S3 Glacier retrieval options. 22/10/2025 22/10/2025 https://www.youtube.com/watch?v=mPBjB6Ltl_Q\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=105[3] THU - Watch follow‚Äëup S3 module video (e.g., ‚ÄúModule 04‚Äë04 ‚Äì S3 Versioning \u0026amp; Security / Data Protection‚Äù).\n- Practice in console: enable versioning on a test bucket, upload multiple versions of an object, test delete/restore behavior and observe how versioning helps against accidental delete/overwrite. 23/10/2025 23/10/2025 https://www.youtube.com/watch?v=YXn8Q_Hpsu4\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=106[1] FRI - Rest day: no new labs or videos. 24/10/2025 24/10/2025 Week 6 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material MON - Review AWS Well-Architected Framework: 6 pillars and how they relate to security, reliability, performance, and cost optimization.- Skim core services overview: EC2, S3, IAM, RDS, VPC, Lambda, CloudWatch, CloudFront and map each one to the relevant pillars. 27/10/2025 27/10/2025 TUE - Secure Architectures: revise IAM (users, roles, policies), MFA, SCP, KMS encryption, TLS/ACM, Security Groups vs NACLs, WAF, Shield, GuardDuty, Secrets Manager. 28/10/2025 28/10/2025 WED - Resilient Architectures: revise Multi-AZ vs Multi-Region, RDS Multi-AZ, DR strategies (backup \u0026amp; restore, pilot light, warm standby, multi-site), Auto Scaling, ELB, Route 53 health checks, backup \u0026amp; restore patterns with S3/Backup. 29/10/2025 29/10/2025 THU - High-Performing \u0026amp; Cost-Optimized Architectures: review compute scaling (EC2 Auto Scaling, Lambda, Fargate), storage options (S3/EFS/EBS) and caching; then revise cost tools (Cost Explorer, Budgets, Savings Plans/RI, lifecycle policies, NAT optimization, storage tiering). 30/10/2025 30/10/2025 FRI -Go to the exam 31/10/2025 31/10/2025 Week 7 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material MON - Refine business requirements and user stories for Aurora Time (scheduling, reminders, authentication, email notifications).- Define non-functional requirements: scalability, availability, latency, cost constraints. 03/11/2025 03/11/2025 Proposal document (Executive Summary, Problem Statement) TUE - Design high-level serverless architecture: API Gateway + Lambda + DynamoDB + EventBridge + SES + Cognito + Amplify + S3/CloudFront.- Draw architecture diagram and align with the ‚ÄúSolution Architecture‚Äù section of the proposal. 04/11/2025 04/11/2025 Proposal ‚Äì Solution Architecture \u0026amp; AWS Services Used WED - DynamoDB data modeling: design tables/PK/SK for users, events, schedules, reminder rules (consider access patterns: list events, create/update/delete, query by date/user).- Document partition key/sort key choices and expected RCU/WCU patterns. 05/11/2025 05/11/2025 Proposal ‚Äì Technical Implementation Plan (DynamoDB modeling) THU - Define API contract for backend: list all REST endpoints (e.g. /events, /events/{id}, /reminders) with methods, request/response schema, error codes.- Map each endpoint to Lambda functions and identify required IAM permissions. 06/11/2025 06/11/2025 Proposal ‚Äì AWS Lambda, API Gateway, DynamoDB roles FRI - Attend an AWS event 07/11/2025 07/11/2025 Week 8 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material MON - Set up basic Cognito User Pool and test user sign-up/sign-in flow (hosted UI or simple test client).- Verify token structure and how APIs will read user identity/claims. 10/11/2025 10/11/2025 Proposal ‚Äì Cognito \u0026amp; Authentication TUE - Create initial DynamoDB table(s) according to Week 1 model and perform simple CRUD tests via AWS Console/CLI.- Validate partition key design with sample workloads and note hot-partition risks. 11/11/2025 11/11/2025 Proposal ‚Äì DynamoDB usage \u0026amp; data model plan WED - Build a minimal POC: API Gateway + 1‚Äì2 Lambda functions (e.g. CreateEvent, ListEvents) integrated with DynamoDB.- Test end-to-end flow (HTTP request ‚Üí API Gateway ‚Üí Lambda ‚Üí DynamoDB) using test data. 12/11/2025 12/11/2025 Proposal ‚Äì Serverless architecture (API Gateway, Lambda, DynamoDB) THU - Configure EventBridge rule + Lambda/SES POC for sending reminder emails on a schedule (simple test rule and template).- Validate SES sending limits and verify sender emails/domains. 13/11/2025 13/11/2025 Proposal ‚Äì EventBridge \u0026amp; SES reminders FRI - Use AWS Pricing Calculator to estimate monthly cost for key services (API Gateway, Lambda, DynamoDB, S3, CloudFront, Amplify, Cognito, SES, EventBridge, CloudWatch). 14/11/2025 14/11/2025 Proposal ‚Äì Estimated Infrastructure Cost table Week 9 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/5-workshop/5.5-frontend/5.5.2-amplify/-index/","title":"Deploy Amplify &amp; Integration","tags":[],"description":"","content":"1. Deploy Frontend with Amplify Go to AWS Amplify \u0026gt; Create new app. Select GitHub \u0026gt; Select the Repository containing your React code. Build settings: Amplify automatically detects npm run build. Click Save and Deploy. Illustration: "},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/4-eventparticipated/4.2-event2/","title":"Discover Agentic AI ‚Äì Amazon QuickSuite Workshop","tags":[],"description":"","content":"Discover Agentic AI ‚Äì Amazon QuickSuite Workshop - Date: November 7, 2025 - Location: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nEvent Overview An exclusive workshop focused on the shift from passive Generative AI to autonomous Agentic AI. The event featured the first live demonstration of Amazon QuickSuite in Vietnam and introduced the AWS LIFT Program to lower financial barriers for adoption.\nKey Objectives:\nDefine Agentic AI: Clarify the concept of autonomous AI agents that can reason and execute tasks. Introduce Amazon QuickSuite: Showcase the unified data visualization (QuickSight) and generative AI (Quick Suite Q) platform. Enable Hands-on Learning: Provide a practical environment to build AI concepts with expert guidance. Facilitate Adoption: Offer an $80,000 USD credit through the AWS LIFT Program to accelerate R\u0026amp;D. Key Takeaways \u0026amp; Learnings Focus on Autonomy: The design goal of Agentic AI is to build systems that act on a user\u0026rsquo;s behalf, not just provide information. Ecosystem Approach is Crucial: Effective agents require a connected network of tools, like the one provided by QuickSuite, to link data sources with action logic. Early Adoption Creates Advantage: Gaining proficiency with tools like QuickSuite before they become mainstream offers a significant competitive edge. Funding Accelerates Innovation: Financial incentives like the LIFT program enable companies to experiment and innovate more quickly. Application to Work Explore QuickSuite for Analytics: Investigate integrating QuickSight and Quick Suite Q to create \u0026ldquo;Analyst Agents\u0026rdquo; that can automate data reporting and analysis. Secure R\u0026amp;D Funding: Apply for the AWS LIFT Program to secure credits for upcoming AI-related research and development projects. Identify Automation Use Cases: Audit internal operations to find repetitive, multi-step tasks suitable for autonomous execution by an AI agent. Engage with Implementation Partners: Collaborate with partners like Cloud Kinetics for complex architectural design and implementation, reducing in-house development risks. Event Photos Add your event photos here\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Aurora Time Executive Summary This proposal presents the implementation plan for Aurora Time, a time-management application built on AWS.\nThe goal is to provide a simple, intuitive, and cost-efficient scheduling tool for individual users.\nAurora Time leverages Serverless and Managed Services on AWS to achieve scalability, high reliability, and cost optimization ‚Äî delivering rapid return on investment (ROI) by eliminating infrastructure maintenance overhead.\nProblem Statement Current Problems Users struggle to manage their personal schedules because information is scattered across many tools (notes, phones, various apps).\nExisting solutions are often overly complex, business-oriented, and not suited for individual use.\nAurora Time provides a centralized, minimalistic, and intuitive solution that helps users easily manage habits and personal events.\nProposed Solution Aurora Time uses Amazon S3 and CloudFront to store and distribute the web application, and AWS Amplify for quick deployment.\nAmazon API Gateway and AWS Lambda handle backend CRUD operations, while DynamoDB provides fast and stable data storage.\nAmazon Cognito handles user authentication, and EventBridge \u0026amp; SES send automated reminders.\nKey Features:\nIntuitive scheduling interface Custom reminders Extremely low cost Benefits and Return on Investment (ROI) Aurora Time helps users save time, reduce scattered scheduling, and increase productivity.\nInfrastructure cost: $16 ‚Äì $50/month (~$192 ‚Äì $600/year) ROI: Under 6 months Advantages: No servers needed, ultra-low cost, highly scalable Solution Architecture Aurora Time applies an AWS Serverless architecture that allows flexible scaling from a single user to millions.\nRequests are received by Amazon API Gateway, processed by AWS Lambda, and stored in DynamoDB.\nEventBridge schedules and triggers reminders, AWS Amplify hosts the frontend, and Cognito provides security.\nAWS Services Used AWS Service Function AWS Lambda Handles CRUD business logic and reminders. Amazon API Gateway Provides secure RESTful APIs. Amazon DynamoDB Stores scheduling, event, and user data. Amazon S3 \u0026amp; CloudFront Stores and distributes the frontend content. Amazon EventBridge Schedules and triggers automated events. Amazon SES Sends reminder emails to users. AWS Amplify Hosts and manages the frontend. Amazon Cognito Secure user authentication and management. Technical Implementation Plan January ‚Äì Research \u0026amp; Architecture Design:\nDynamoDB modeling, Serverless architecture (API Gateway, Lambda, EventBridge). January ‚Äì POC \u0026amp; Cost Estimation:\nAWS Pricing Calculator, testing Cognito + DynamoDB. February ‚Äì System Optimization:\nTune Lambda (timeout, memory), optimize DynamoDB RCU/WCU. February‚ÄìMarch ‚Äì Development \u0026amp; CI/CD:\nBuild Lambda functions, set up CodePipeline + CodeBuild, develop React UI, Beta testing. Estimated Infrastructure Cost Service Description Monthly Cost (USD) AWS Amplify Static web hosting 0.35 S3 Static files \u0026amp; backup 0.05 CloudFront CDN (20GB) 1.70 API Gateway 30,000 requests 0.11 Lambda 1M requests (free tier) 0.00 DynamoDB 1GB data (free tier) 0.11 Cognito \u0026lt;1000 users 0.00 SES 500 emails/month 0.05 EventBridge 100k events 0.10 CloudWatch Logs 1GB logs 0.10 CI/CD Pipeline 20 builds 0.00 üëâ Total Estimated Cost: ~ $16 ‚Äì $50/month (~$192 ‚Äì $600/year)\nRisks \u0026amp; Mitigation Risk Impact Probability Mitigation Network outage Medium Medium Use cache \u0026amp; CDN (CloudFront). DynamoDB failure High Medium Conduct POC \u0026amp; load testing. Cost overrun Medium Low Set up AWS Budgets alerts. EventBridge/Lambda failure High Low Monitor via CloudWatch \u0026amp; retry logic. Expected Outcomes Simple, intuitive scheduling with automated reminders. Stable, low-cost, scalable serverless system. Full deployment during internship with automated CI/CD. "},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material MON - Refine business requirements and user stories for Aurora Time (scheduling, reminders, authentication, email notifications).- Define non‚Äëfunctional requirements: scalability, availability, latency, and cost constraints. 17/11/2025 17/11/2025 Proposal ‚Äì Executive Summary, Problem Statement TUE - Design high-level serverless architecture: API Gateway, Lambda, DynamoDB, EventBridge, SES, Cognito, Amplify, S3/CloudFront.- Draw/update the architecture diagram based on the proposal. 18/11/2025 18/11/2025 Proposal ‚Äì Solution Architecture \u0026amp; AWS Services Used WED - DynamoDB data modeling: design tables, partition key and sort key for users, events, schedules, and reminders.- Document access patterns (create/update/delete event, list by date/user, query upcoming reminders). 19/11/2025 19/11/2025 Proposal ‚Äì Technical Implementation Plan (DynamoDB modeling) THU Go to event 20/11/2025 20/11/2025 FRI - Draft security \u0026amp; authentication design: Cognito user pools, auth flow, JWT with API Gateway authorizer, basic authorization rules.- Write \u0026ldquo;Architecture Design v1\u0026rdquo; (short document) and share with mentor/team for feedback. 21/11/2025 21/11/2025 Proposal ‚Äì Cognito, Security, ROI \u0026amp; Risks sections Week 10 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material MON - Set up Cognito User Pool and test sign‚Äëup/sign‚Äëin flow (hosted UI or simple client).- Inspect ID/Access tokens and plan how backend will read user identity and roles. 24/11/2025 24/11/2025 Proposal ‚Äì Amazon Cognito \u0026amp; Authentication TUE - Create initial DynamoDB table(s) according to Week 1 model and perform simple CRUD tests via Console/CLI.- Validate partition key design with sample data, note potential hot‚Äëpartition risks. 25/11/2025 25/11/2025 Proposal ‚Äì DynamoDB usage \u0026amp; data model plan WED - Build minimal POC: API Gateway + a few Lambda functions (e.g. CreateEvent, ListEvents) integrated with DynamoDB.- Test end‚Äëto‚Äëend flow (HTTP request ‚Üí API Gateway ‚Üí Lambda ‚Üí DynamoDB) with sample payloads. 26/11/2025 26/11/2025 Proposal ‚Äì Serverless architecture (API Gateway, Lambda, DynamoDB) THU - Configure EventBridge rule plus Lambda/SES POC for sending reminder emails on a schedule.- Verify SES identities, limits, and send at least one test reminder email successfully. 27/11/2025 27/11/2025 Proposal ‚Äì EventBridge \u0026amp; SES reminders FRI - Use AWS Pricing Calculator to estimate monthly costs for API Gateway, Lambda, DynamoDB, S3, CloudFront, Amplify, Cognito, SES, EventBridge, CloudWatch. 28/11/2025 28/11/2025 Proposal ‚Äì Estimated Infrastructure Cost table Week 11 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Mon - Perform a full review of all features: authentication, event CRUD, reminders, scheduled emails.\n- Check CloudWatch logs for remaining errors and verify system correctness. 01/12/2025 01/12/2025 Project Checklist Tue - Execute End-to-End Testing: sign-up ‚Üí sign-in ‚Üí create/edit/delete events ‚Üí reminder scheduling ‚Üí email delivery ‚Üí cross-device validation.\n- Identify issues and fix remaining bugs. 02/12/2025 02/12/2025 Test Cases ‚Äì Aurora Time Wed - Polish final UI/UX of the web application: layout, spacing, responsiveness, validation rules, error messages.\n- Capture final screenshots for the slide deck and final report. 03/12/2025 03/12/2025 UI Review Notes Thu - Prepare the Final Presentation Slide Deck: problem statement, system architecture, workflows, demo flow, cost estimation, benefits, lessons learned.\n- Write the Demo Script to ensure a smooth and consistent presentation. 04/12/2025 04/12/2025 Presentation Materials Fri - Re-run system validation using the final checklist.\n- Complete all deliverables: Final Report, Architecture Diagram, Cost Estimation Sheet, Demo Video, and supporting materials.\n- Document Lessons Learned \u0026amp; Next Steps. 05/12/2025 05/12/2025 Final Deliverables Week 12 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/5-workshop/5.3-database/","title":"Database Design (Events &amp; Tasks)","tags":[],"description":"","content":" üóÑÔ∏è Objective: Design a NoSQL database using Amazon DynamoDB to store Events and Todo tasks, optimized for fast retrieval and low operational cost.\n1. Why Choose Amazon DynamoDB? With Aurora‚Äôs Serverless architecture, Amazon DynamoDB is the ideal choice because:\nServerless: No server management required, automatically scales based on workload. High performance: Low latency (single-digit milliseconds), ideal for real-time UI operations. Flexible (Schemaless): Easy to add new fields to Event/Todo items without complex migrations like SQL databases. 2. Schema Design (Data Modeling) The system follows a Per-User Isolation model.\nEvery item is associated with a userId (extracted from Cognito/Google tokens) to ensure data security.\nWe will create three main tables:\nTable 1: AuroraEvents (Stores calendar events) This table stores user events, displayed on the calendar and used for notification scanning.\nPartition Key (PK): userId (String) ‚Äî User identifier Sort Key (SK): eventId (String) ‚Äî UUID of the event Table 2: AuroraTasks (Stores daily tasks) This table stores the user‚Äôs Daily Worklog / Todo list.\nPartition Key (PK): userId (String) Sort Key (SK): todoId (String) ‚Äî UUID of the task Table 3: users (Stores user information) This table contains detailed information about each user.\nPartition Key (PK): userId (String) 3. Steps to Create Tables on AWS Console Below are the steps to create tables using the AWS DynamoDB Console.\nStep 1: Create the Events Table Navigate to DynamoDB ‚Üí Tables ‚Üí Create table.\nTable name: events Partition key: userId (String) Sort key: eventId (String) Screenshot:\nStep 2: Create the Tasks Table Repeat the process to create the Tasks table.\nTable name: todo Partition key: userId (String) Sort key: todoId (String) Screenshot:\nStep 3: Create the Users Table Finally, create the Users table.\nTable name: users Partition key: userId (String) Screenshot:\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/5-workshop/5.4-backend-logic/5.4.3-sendreminder/","title":"Send Reminder (Notification)","tags":[],"description":"","content":" üîî Function: This function is designed to run on a schedule (e.g., every 5 minutes using EventBridge Scheduler). It scans the database for events happening within the next 15 minutes and sends reminder emails via a third-party API.\nStep 1: Create Lambda Function This function requires a slightly longer execution time since it needs to scan the database and wait for responses from the email API, so we will increase the Timeout.\nFunction name: sendReminder Runtime: Node.js 18.x Illustration:\nFigure 5.4.3.1: Configuring the background job Lambda function.\nStep 2: Configure IAM Role (Full Access to Events) JSON Policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:ap-southeast-1:080563425614:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:ap-southeast-1:080563425614:log-group:/aws/lambda/sendReminderLambda:*\u0026#34; ] } ] } Step 3: Implementation Code (Node.js) Return to the Lambda Function interface, where we will write Node.js code to scan for upcoming events and send reminder emails via a third-party API.\nImage: After completing the code, click Deploy to save it.\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/4-eventparticipated/4.3-event3/","title":"AWS Cloud Mastery Series #3 - Security Pillar Deep Dive","tags":[],"description":"","content":"AWS Cloud Mastery Series #3 - Security Pillar Deep Dive - Date: December 1, 2025 - Location: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nEvent Overview An in-depth workshop focused on the Security Pillar of the AWS Well-Architected Framework. The event provided knowledge and best practices for securing cloud workloads.\nKey Objectives:\nDeep Dive into the Security Pillar: Analyze the design principles and key areas of security on AWS. Identity and Access Management: Gain a deep understanding of AWS IAM, MFA, and best practices for access control. Data Protection: Explore techniques for encrypting data at-rest and in-transit. Automation and Monitoring: Learn how to use AWS Config, CloudTrail, and Security Hub to monitor and automate security controls. Key Takeaways \u0026amp; Learnings Security is a Shared Responsibility: Understand the shared responsibility model and the customer\u0026rsquo;s role in securing applications in the cloud. Defense in Depth: Apply multiple layers of security to protect resources comprehensively. Automation is Key: Automating security checks and remediation reduces human error and allows for faster responses to threats. Application to Work Re-evaluate IAM Policies: Review and strengthen existing IAM policies according to the principle of least privilege. Implement Security Monitoring: Set up AWS Security Hub for a centralized, comprehensive view of the security posture. Enhance Data Encryption: Ensure all sensitive data is encrypted using AWS KMS. "},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\n(3.1-Blog1/) This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/5-workshop/5.4-backend-logic/","title":"Backend-Logic","tags":[],"description":"","content":" üõ°Ô∏è Goal: Build a Serverless Backend following a 4-step workflow: Create Function ‚Üí Configure Role ‚Üí Grant Database Permissions ‚Üí Implement Logic (Connect to DB \u0026amp; Call Third-Party Email API).\nStep 1: Initialize the Lambda Function First, we create a new Lambda function that will contain the processing logic.\nGo to AWS Console \u0026gt; Lambda \u0026gt; Create function. Function name: (enter your function name). Runtime: Choose Node.js 18.x (or 20.x). Architecture: x86_64. Keep the remaining settings as default and click Create function. Illustration:\nFigure 5.4.1: Creating the Backend processing Lambda function.\nStep 2: Add Policies to the Lambda (Execution Role) When the function is created, AWS automatically generates a basic IAM Role.\nWe need to access this Role and add permissions for writing to the Database.\nIn the Lambda function page, switch to the Configuration tab. Select Permissions on the left panel. Click the Role name under Execution role to open it in the IAM Console. Illustration:\nFigure 5.4.2: Accessing the IAM Role for permission configuration.\nStep 3: Add Policies for Database Access Since we use a third-party Email service (called via a normal HTTP API), we do not need SES permissions.\nWe only need to grant Lambda permissions to work with DynamoDB.\nIn the Role‚Äôs Permissions tab, click Add permissions \u0026gt; Create inline policy. Switch to the JSON mode. Click Next, name the policy AuroraDB_Access_Policy, and then click Create policy. Review your Permissions list to ensure the Role now has access to DynamoDB. Step 4: Write the Lambda Function Code Return to the Lambda Function interface and begin writing your Node.js code. Once completed, click Deploy to save and apply the changes.\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3‚Ä¶, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event‚Äôs content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Cloud Day Vietnam - AI Edition 2025\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: Discover Agentic AI ‚Äì Amazon QuickSuite Workshop\nDate \u0026amp; Time: 09:00, November 7, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Cloud Mastery Series #3 - Security Pillar Deep Dive\nDate \u0026amp; Time: 09:00, December 1, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/5-workshop/","title":"Workshop","tags":[],"description":"","content":" üí° Project Information: Aurora is an event-management and automated notification system built on a Serverless architecture.\nProject Aurora: Event Management \u0026amp; Automated Notification System Project Overview Project Aurora is a solution that enables users to manage schedules, create important events, and track daily tasks (Daily Worklog).\nA key highlight of the system is its ability to integrate automated email notifications, reminding users when an event is about to occur.\nThis project focuses on solving the problem of creating schedules and delivering reliable notifications without maintaining traditional servers.\nKey features and services include:\nAmazon Cognito \u0026amp; Google Cloud: Centralized user identity management with Google Sign-In (OAuth 2.0) for secure and simplified authentication. Event \u0026amp; Task Management: Built using Amazon DynamoDB to store event details and daily task statuses. Email Notification System: Integrated with Resend to deliver beautiful HTML-formatted emails to users. Logic Processing: Implemented with AWS Lambda to handle workflow execution whenever a new event is created. Detailed Sections System Architecture \u0026amp; Authentication Flow Google Cloud \u0026amp; Amazon Cognito Configuration Database Design (Events \u0026amp; Tasks) Building API \u0026amp; Email Logic (Lambda) User Interface (Frontend) Final Results \u0026amp; Future Improvements "},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/5-workshop/5.5-frontend/","title":"Frontend &amp; API Gateway","tags":[],"description":"","content":" üåê Architecture: We will build a standard secure system: Frontend (Amplify) -\u0026gt; API Gateway (with Cognito authentication) -\u0026gt; Lambda Backend.\nConnection Model To ensure security and centralized management, the Frontend is not allowed to call Lambda directly. Instead, we use Amazon API Gateway.\nWorkflow:\nDeploy: The ReactJS Frontend is hosted on AWS Amplify. Authenticate: Users log in via Google/Cognito and receive an IdToken. Request: The Frontend sends a request to API Gateway with the Authorization Header containing the Token. Authorize: API Gateway verifies the Token with Cognito. If valid -\u0026gt; forwards the request to Lambda. Execute: Lambda executes the logic and returns the result. "},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During the internship at AWS from 08/09/2025 to 24/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired at university in a real working environment. I participated in Aurora Time, a time-management application built on AWS serverless architecture that helps individual users easily schedule events, set reminders, and manage personal habits with a simple, low-cost interface, thereby improving my skills in AWS architecture analysis \u0026amp; design, serverless backend development (Lambda, API Gateway, DynamoDB), cloud infrastructure deployment (IAM, VPC, EventBridge, CloudFront, Amplify), teamwork, and communication in a project environment.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚òê ‚úÖ ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚òê ‚úÖ ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚úÖ ‚òê 6 Progressive mindset Willingness to receive feedback and improve oneself ‚úÖ ‚òê ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚òê ‚úÖ ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions Throughout your internship, which experience or achievement made you feel the most proud and satisfied? Which moment (project, task, or event) made you feel that ‚ÄúI truly belong to this team‚Äù? Why? -In your opinion, which parts of the internship program could be organized better (onboarding, task assignment, mentoring, feedback, etc.)? Suggestions \u0026amp; Expectations Do you have any specific suggestions to improve the internship experience (for example, more sharing sessions, pair programming, code reviews, or soft-skill workshops)? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://sangnvse183276.github.io/fcj-workshop-template/tags/","title":"Tags","tags":[],"description":"","content":""}]